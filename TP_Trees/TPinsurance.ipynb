{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "TP-insurance.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSCS97phw2rG"
      },
      "source": [
        "# Medical insurance prediction\n",
        "\n",
        "**Goal:** Predict the insurance costs using all available features. \n",
        "\n",
        "Credits: Data freely available from https://github.com/stedy/Machine-Learning-with-R-datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh9NnIOyoRVI"
      },
      "source": [
        "Let's load the dataset and the needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AiupRSMw2rQ"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zx258wW7w5D6"
      },
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "  from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "  gdd.download_file_from_google_drive(file_id='1xhM2x8WoIk3JicauwfQrhKmC1_kZ6KzX',\n",
        "  dest_path='./insurance.csv')\n",
        "else:\n",
        "  print('Please put the file insurance.csv in the same folder as the notebook')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wje3xIfHx7EK"
      },
      "source": [
        "data_original = pd.read_csv('./insurance.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CJJd-vXw2rt"
      },
      "source": [
        "Let's look at the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRM-z9sXw2r2"
      },
      "source": [
        "data_original.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVldeOetw2sH"
      },
      "source": [
        "So we have seven features:\n",
        "- age: age of primary beneficiary\n",
        "- sex: insurance contractor gender, female, male\n",
        "- bmi: Body mass index, providing an understanding of body, weights that are relatively high or low relative to height, objective index of body weight (kg / m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9\n",
        "- children: Number of children covered by health insurance / Number of dependents\n",
        "- smoker: yes or no\n",
        "- region: the beneficiary's residential area in the US, northeast, southeast, southwest, northwest.\n",
        "- charges: Individual medical costs billed by health insurance. That's what we want to predict !\n",
        "\n",
        "Remember that our goal is to predict the `charges` using all the other features. So `charges` represents our vector of `y `and all other features our matrix `X`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-KKETsew2sL"
      },
      "source": [
        "*Question*: Is it a regression or a classification problem ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2jnN-Kgw2sN"
      },
      "source": [
        "The first thing to do is to look if there are `Nan`values, namely errors or missing values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-Swdpdxw2sQ"
      },
      "source": [
        "data_original.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-Mn8bxbw2sb"
      },
      "source": [
        "Good news ! No `Nan` values\n",
        "\n",
        "Now let's have a look at the distribution of the different features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ga-Ta0bhw2se"
      },
      "source": [
        "n_bins = 20\n",
        "fig, axs = plt.subplots(2, 4, sharey=False, tight_layout=True, figsize=(15, 5))\n",
        "\n",
        "axs[0, 0].hist(data_original.age,bins=50)\n",
        "axs[0, 0].set_title('age')\n",
        "axs[0, 1].hist(data_original.sex)\n",
        "axs[0, 1].set_title('sex')\n",
        "axs[0, 2].hist(data_original.bmi,bins=50)\n",
        "axs[0, 2].set_title('bmi')\n",
        "axs[0, 3].hist(data_original.children)\n",
        "axs[0, 3].set_title('children')\n",
        "axs[1, 0].hist(data_original.smoker)\n",
        "axs[1, 0].set_title('smoker')\n",
        "axs[1, 1].hist(data_original.region)\n",
        "axs[1, 1].set_title('region')\n",
        "axs[1, 2].hist(data_original.charges,bins=50)\n",
        "axs[1, 2].set_title('charges')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEOXgcKaw2ss"
      },
      "source": [
        "We can already notice that:\n",
        "- distribution of age is quite uniform\n",
        "- male and female are balanced\n",
        "- bmi looks like a Gaussian distribution (good for stats)\n",
        "- There are definitely more people without children or with only one\n",
        "- There are definitely more people who don't smoke\n",
        "- regions are balanced\n",
        "- charges is a right skewed distribution -> we can use a log to make it normal ! Easy to use in practice, many models and statistical test make the normal distribtion hypothesis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWXSACGVw2su"
      },
      "source": [
        "It seems that we have both numerical and categorical features. Let's check that !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSWjBIMyw2sx"
      },
      "source": [
        "data_original.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyNL-DzCw2s-"
      },
      "source": [
        "OK. we have 3 categorical variables. Let's convert them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcCb1qBJw2s_"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "data = data_original.copy()\n",
        "\n",
        "# Sex\n",
        "data.sex[data.sex=='female']=0\n",
        "data.sex[data.sex=='male']=1\n",
        "data.sex = data.sex.astype(str).astype(int)\n",
        "\n",
        "# same as\n",
        "#le = LabelEncoder()\n",
        "#data.sex = le.fit_transform(data.sex)\n",
        "\n",
        "# Smoker\n",
        "# here it's better to do it by hand to be sure that 'no' is 0 and 'yes' is 1\n",
        "data.smoker[data.smoker=='no']=0\n",
        "data.smoker[data.smoker=='yes']=1\n",
        "data.smoker = data.smoker.astype(str).astype(int)\n",
        "\n",
        "# Region\n",
        "enc = OneHotEncoder(sparse=False) # better not to use sparse matrix here (more interpretable)\n",
        "cat_regions = data.region.unique() # list of regions\n",
        "# OneHotEncoder needs a 2D array, so we need to transform it in a column vector \n",
        "# and in order to do that we need to have a numpy array, thus the .values\n",
        "region = enc.fit_transform(data.region.values.reshape(-1, 1))\n",
        "\n",
        "# region is now a numpy array, needs to transofmr in a panda frame\n",
        "region_df = pd.DataFrame(region, columns=cat_regions)\n",
        "\n",
        "# we can now drop the column with the old 'region' feature ...\n",
        "data.drop('region',inplace=True, axis=1)\n",
        "\n",
        "# ... and add the new binary variables resulting from OneHotEncoder\n",
        "dataOHE = pd.concat([data, region_df], axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijM46P3sw2tH"
      },
      "source": [
        "Let's have a look at the new DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7AmOwJyw2tJ"
      },
      "source": [
        "dataOHE.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUN0lrtgw2tX"
      },
      "source": [
        "OK. All variables are numerical. We can now start the analysis. It's always good to start by checking the correlations between the variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Up2mw5SRw2tc"
      },
      "source": [
        "f, ax = plt.subplots(figsize=(10, 8))\n",
        "corr = dataOHE.corr()\n",
        "sns.heatmap(corr,square=True,ax=ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvfIYjMfw2tm"
      },
      "source": [
        "We can see that:\n",
        "- bmi seem to be a bit correlated with the nortwest region and with charges\n",
        "- all the other high correlations concern `charges`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMaz0yv3w2tn"
      },
      "source": [
        "To see possible (direct) colinearities among features the scatter plot is also a good tool"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Se69zsRw2tp"
      },
      "source": [
        "axeslist = pd.plotting.scatter_matrix(dataOHE, diagonal='hist', figsize=(10,10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-TVmqE0w2tz"
      },
      "source": [
        "We can see that there is a clear pattern, like three different cluster, between age and charges. Let's have a closer look"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqKA9dJAw2t1"
      },
      "source": [
        "fig1 = plt.figure(figsize=(6, 6))\n",
        "ax = plt.gca()\n",
        "plt.scatter(dataOHE.age, dataOHE.charges, c='b',s=80, marker='o')\n",
        "plt.ylabel('charges')\n",
        "plt.xlabel('age')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXOQ1gsCw2t_"
      },
      "source": [
        "We'll come back to that later.\n",
        "Now let's have a closer look to the correlations of `charges`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spgw-x4Iw2uC"
      },
      "source": [
        "dataOHE.corr()['charges'].sort_values()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEV_vlg5w2uM"
      },
      "source": [
        "Well, guess what... it seems that if you smoke, your charges are higher. \n",
        "What happens if we change the distribution of charges using the log function ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFTWXVVhw2uO"
      },
      "source": [
        "charge_dist = dataOHE[\"charges\"].values\n",
        "logcharge = np.log(dataOHE[\"charges\"].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTfWF_cow2ub"
      },
      "source": [
        "n_bins = 20\n",
        "fig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\n",
        "\n",
        "axs[0].hist(charge_dist, bins=n_bins)\n",
        "axs[0].set_title('charges')\n",
        "axs[1].hist(logcharge, bins=n_bins)\n",
        "axs[1].set_title('ln(charges)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoeSPNcCw2uk"
      },
      "source": [
        "dataOHE.charges=np.log(dataOHE.charges)\n",
        "dataOHE.corr()['charges'].sort_values()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L32j9Ihbw2ur"
      },
      "source": [
        "Well, now it seems that charges is also correalated a bit with the age, which is quite expected I would say"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXWda_fdw2ut"
      },
      "source": [
        "Let's look at the distribution of charges for smokers and non smokers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TjmlTqNw2uv"
      },
      "source": [
        "f= plt.figure(figsize=(12,5))\n",
        "\n",
        "ax=f.add_subplot(121)\n",
        "sns.distplot(dataOHE[(dataOHE.smoker == 1)][\"charges\"],color='c',ax=ax)\n",
        "ax.set_title('Distribution of charges for smokers')\n",
        "\n",
        "ax=f.add_subplot(122)\n",
        "sns.distplot(dataOHE[(dataOHE.smoker == 0)]['charges'],color='b',ax=ax)\n",
        "ax.set_title('Distribution of charges for non-smokers')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu6bTbVkw2u7"
      },
      "source": [
        "We can see that non-smokers pay less but let's look if we also condition on the age"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sDJtzOtw2u9"
      },
      "source": [
        "g = sns.jointplot(x='age', y='charges',data=dataOHE[dataOHE.smoker==0], color=\"m\")\n",
        "g.set_axis_labels(\"$age$\", \"$charges$\")\n",
        "ax.set_title('Distribution of charges and age for non-smokers')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDGUk_6vw2vE"
      },
      "source": [
        "Well, it seems that for non-smokers charges increase with age. Quite expected !\n",
        "Let's look now at smokers..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qV46Vlxw2vG"
      },
      "source": [
        "g = sns.jointplot(x='age', y='charges',data=dataOHE[(dataOHE.smoker==1)], color=\"m\")\n",
        "g.set_axis_labels(\"$age$\", \"$charges$\")\n",
        "ax.set_title('Distribution of charges and age for smokers')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5TVgSZTw2vR"
      },
      "source": [
        "We can see that there are clearly two trends... might it be the sex ? Let's have a look"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEbpvfxZw2vU"
      },
      "source": [
        "g = sns.jointplot(x='age', y='charges',data=dataOHE[(dataOHE.smoker==1) & (dataOHE.sex==0)], color=\"m\")\n",
        "g.set_axis_labels(\"$age$\", \"$charges$\")\n",
        "ax.set_title('Distribution of charges and age for female smokers')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pagYzlFdw2vb"
      },
      "source": [
        "Nope, wrong hypothesis... let's try with bmi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvDRmwz4w2vc"
      },
      "source": [
        "fig1 = plt.figure(figsize=(6, 6))\n",
        "ax = plt.gca()\n",
        "\n",
        "plt.scatter(dataOHE[(dataOHE.smoker==1) & (dataOHE.bmi<30)].age, dataOHE[(dataOHE.smoker==1) & (dataOHE.bmi<30)].charges,\n",
        "                c='b',s=80, marker='o',label='smoking thin')\n",
        "plt.scatter(dataOHE[(dataOHE.smoker==1) & (dataOHE.bmi>=30)].age, dataOHE[(dataOHE.smoker==1) & (dataOHE.bmi>=30)].charges,\n",
        "                c='r',s=80, marker='x',label='smoking obese')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.ylabel('charges')\n",
        "plt.xlabel('age')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QL_vMGo1w2vk"
      },
      "source": [
        "Bingo ! We can clearly see that, among smoker, obese people pay more than thin people regardless the age"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shl7IBDzw2vm"
      },
      "source": [
        "And what if we also consider the non-smoking ones ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYXbdDfvw2vo"
      },
      "source": [
        "fig1 = plt.figure(figsize=(9, 9))\n",
        "ax = plt.gca()\n",
        "\n",
        "plt.scatter(dataOHE[(dataOHE.smoker==1) & (dataOHE.bmi<30)].age, dataOHE[(dataOHE.smoker==1) & (dataOHE.bmi<30)].charges,\n",
        "                c='b',s=80, marker='o',label='smoking thin')\n",
        "plt.scatter(dataOHE[(dataOHE.smoker==1) & (dataOHE.bmi>=30)].age, dataOHE[(dataOHE.smoker==1) & (dataOHE.bmi>=30)].charges,\n",
        "                c='r',s=80, marker='x',label='smoking obese')\n",
        "plt.scatter(dataOHE[(dataOHE.smoker==0) & (dataOHE.bmi<30)].age, dataOHE[(dataOHE.smoker==0) & (dataOHE.bmi<30)].charges,\n",
        "                c='g',s=80, marker='s',label='non smoking thin')\n",
        "plt.scatter(dataOHE[(dataOHE.smoker==0) & (dataOHE.bmi>=30)].age, dataOHE[(dataOHE.smoker==0) & (dataOHE.bmi>=30)].charges,\n",
        "                c='y',s=80, marker='p',label='non smoking obese')\n",
        "\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.ylabel('charges')\n",
        "plt.xlabel('age')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBeV_UGTw2vy"
      },
      "source": [
        "This was not expected ! It seems that if you do not smoke your charges will not change if you are obese or not. Glad to hear it (I am a non smoker..)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz2gACSnw2v1"
      },
      "source": [
        "### ***Prediction***\n",
        "\n",
        "Now it's time to predict the charges using the methods seen during the lectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCGZZSRzw2wA"
      },
      "source": [
        "Let's first create our matrix of features `X`and target vector `y`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXXa6tfIw2wB"
      },
      "source": [
        "y = dataOHE.charges\n",
        "X = dataOHE.drop(['charges'], axis = 1)\n",
        "name_features=X.columns\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3im8DYkaw2wG"
      },
      "source": [
        "We can see that the features `age` and `bmi` have different ranges with respect to the other variables and need to be scaled"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8cJWO2fw2wH"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "n_features = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "\n",
        "print('Number training samples ', n_samples, ' and number features', n_features)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train[['age','bmi']])\n",
        "X_train[['age','bmi']] = scaler.transform(X_train[['age','bmi']])\n",
        "X_test[['age','bmi']] = scaler.transform(X_test[['age','bmi']])\n",
        "\n",
        "# remember to always scale on the train and apply the same scaling factor to the test\n",
        "# you should never use the test set before prediction\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_NqAvJrvKbe"
      },
      "source": [
        "Let's try to use the three linear methods seen in the lecture: linear, Ridge and Lasso."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_L9V8RIn3Iyu"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "\n",
        "# data are already centred and normalized\n",
        "linear = LinearRegression(normalize=False,fit_intercept=False)\n",
        "linear.fit(X_train, y_train)\n",
        "print('Training score: ',linear.score(X_train, y_train), '; Test score: ', linear.score(X_test, y_test))\n",
        "\n",
        "ridge = Ridge(normalize=False,fit_intercept=False)\n",
        "ridge.fit(X_train, y_train)\n",
        "print(ridge.score(X_test, y_test))\n",
        "\n",
        "lasso = Lasso(alpha=1e-3, normalize=False,fit_intercept=False)\n",
        "lasso.fit(X_train, y_train)\n",
        "print(lasso.score(X_test, y_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5g6bxLGkCFMy"
      },
      "source": [
        "It seems that results are not so different. We could use the function `enet_path` to check how parameter weights are changing when using a L2-norm regularization (Ridge) or a L1-norm (Lasso) or both (E-net)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdzMSyMF9i4u"
      },
      "source": [
        "from sklearn.linear_model import enet_path\n",
        "\n",
        "def enet_plot(l1_ratio,X_train,y_train,lambda_lasso,name_features):\n",
        "    \"\"\"Function plotting enet_path for some tuning parameter.\"\"\"\n",
        "    plt.rcParams['mathtext.fontset'] = 'cm'\n",
        "    _, beta_enet, _ = enet_path(X_train, y_train, alphas=lambda_lasso, fit_intercept=False,\n",
        "                                 l1_ratio=l1_ratio, return_models=False)\n",
        "    fig1 = plt.figure(figsize=(15, 5))\n",
        "    ax1 = fig1.add_subplot(111)\n",
        "    ax1.plot(lambda_lasso, np.transpose(beta_enet), linewidth=3)\n",
        "    plt.title(\"Enet path: \" + r\"$p={0}, n={1}, \\gamma={2}$\".format(n_features,\n",
        "                                                        n_samples, l1_ratio), fontsize=16)\n",
        "\n",
        "    plt.legend(name_features,bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    ax1.set_xscale('log')\n",
        "    ax1.set_xlabel(r\"$\\lambda$\")\n",
        "    ax1.set_ylabel(\"Coefficient value\")\n",
        "    \n",
        "    return beta_enet\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7IyqkuKaX2i"
      },
      "source": [
        "#range of lambda values\n",
        "lambda_lasso = np.logspace(np.log10(100), np.log10(1e-2), num=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HK1l0843AbCX"
      },
      "source": [
        "# Ridge (only L2-loss)\n",
        "theta_enet001 = enet_plot(0.00001,X_train,y_train,lambda_lasso,name_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7Q9mzqgAdPp"
      },
      "source": [
        "# E-net (both L2-loss and L1-loss)\n",
        "theta_enet05 = enet_plot(0.5,X_train,y_train,lambda_lasso,name_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHZXTdvfAeh1"
      },
      "source": [
        "# Lasso (only L1-loss)\n",
        "theta_enet099 = enet_plot(0.999,X_train,y_train,lambda_lasso,name_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCyJ6vAjEi4e"
      },
      "source": [
        "It seems that `sex` and `children` are important factors for non-smoking people `smoker=0`and that the four regions have the same importance. What happens if we ignore the region features ? Try to look at that"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBIrAEiKZEWO"
      },
      "source": [
        "X_new_train=X_train.drop(['southwest','northwest','southeast','northeast'], axis = 1)\n",
        "X_new_test=X_test.drop(['southwest','northwest','southeast','northeast'], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRjf9o78ZodW"
      },
      "source": [
        "# Lasso (only L1-loss)\n",
        "theta_enet099 = enet_plot(0.999,X_new_train,y_train,lambda_lasso,X_new_train.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DycqcJqFe3ni"
      },
      "source": [
        "`children`, `sex` and `smoker` seem to be more important than age.\n",
        "Let's try to use other methods with Cross-validation. \n",
        "\n",
        "Remember that also during Cross-validation you should not use test data before the prediction. This means that you should re-fit Standard Scaler at each fold using only the training part and not the test part. In order to be able to do that, you could use a `pipeline`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xG0Oh3yaw2wM"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "# SVR linear\n",
        "scaler = StandardScaler()\n",
        "svr = SVR(kernel='linear')\n",
        "# here we build the pipeline in the right order (first scaling and then fitting)\n",
        "pipe = Pipeline(steps=[('scaler', scaler), ('svr', svr)])\n",
        "# here we choose the hyperparameter of the model\n",
        "parameteres = {'svr__C': [0.001,0.1,1,10]}\n",
        "# here we use GridSearchCV to look for the best hyper-parameters\n",
        "grid = GridSearchCV(pipe, param_grid=parameteres, cv=5)\n",
        "grid.fit(X_train,y_train)\n",
        "# once found the best hyper-parameters we compute the test score\n",
        "print('CV score for linear SVR is :', grid.score(X_test,y_test))\n",
        "print('Best hyperparameters: ',grid.best_params_)\n",
        "\n",
        "\n",
        "# SVR linear\n",
        "scaler = StandardScaler()\n",
        "svr = SVR(kernel='rbf')\n",
        "pipe = Pipeline(steps=[('scaler', scaler), ('svr', svr)])\n",
        "parameteres = {'svr__C': [0.001,0.1,1,10], 'svr__gamma':[0.01,0.1,1,10]}\n",
        "grid = GridSearchCV(pipe, param_grid=parameteres, cv=5)\n",
        "grid.fit(X_train,y_train)\n",
        "print('CV score for rbf SVR is :', grid.score(X_test,y_test))\n",
        "print('Best hyperparameters: ',grid.best_params_)\n",
        "\n",
        "# KNN\n",
        "scaler = StandardScaler()\n",
        "KNN = KNeighborsRegressor()\n",
        "pipe = Pipeline(steps=[('scaler', scaler), ('KNN', KNN)])\n",
        "parameteres = {'KNN__n_neighbors': [2,5,8,10,20]}\n",
        "grid = GridSearchCV(pipe, param_grid=parameteres, cv=5)\n",
        "grid.fit(X_train,y_train)\n",
        "print('CV score for KNN is :', grid.score(X_test,y_test))\n",
        "print('Best hyperparameters: ',grid.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmJNItvVoopl"
      },
      "source": [
        "Let's try to use Decision Trees based methods.\n",
        "For Decision trees, we will focus on two hyperparameters: 'min_samples_split', the minimum number of samples required to split an internal node and 'min_samples_leaf', the minimum number of samples required to be at a leaf node. Other hyperparameters exist. Please have a look at the website of scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0oQ7QTwoG57"
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
        "\n",
        "# Decision Trees\n",
        "scaler = StandardScaler()\n",
        "Tree = DecisionTreeRegressor()\n",
        "pipe = Pipeline(steps=[('scaler', scaler), ('Tree', Tree)])\n",
        "parameteres = {'Tree__min_samples_split': [1,2,5,10,15,20,25], 'Tree__min_samples_leaf': [1,2,5,10,15]}\n",
        "grid = GridSearchCV(pipe, param_grid=parameteres, cv=5)\n",
        "grid.fit(X_train,y_train)\n",
        "print('CV score for decision tree is :', grid.score(X_test,y_test))\n",
        "print('Best hyperparameters: ',grid.best_params_)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaFLRYREoyO6"
      },
      "source": [
        "To plot decision trees, we can also use the *tree* library from scikit-learn.\n",
        "More information at: https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html \n",
        "\n",
        "This makes decision trees highly interpretable. \n",
        "**Question**: which are the most important features ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikqTFHfzovk4"
      },
      "source": [
        "from sklearn import tree\n",
        "\n",
        "Tree = DecisionTreeRegressor(min_samples_split=2, min_samples_leaf=15)\n",
        "Tree.fit(X,y)\n",
        "\n",
        "fig = plt.figure(figsize=(25,20))\n",
        "tree.plot_tree(Tree, feature_names = name_features, max_depth=3, filled = True)\n",
        "#fig.savefig('imagename.png')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvWZamXjokTx"
      },
      "source": [
        "# Bagging\n",
        "scaler = StandardScaler()\n",
        "treeBagging = BaggingRegressor()\n",
        "pipe = Pipeline(steps=[('scaler', scaler), ('treeBagging', treeBagging)])\n",
        "parameteres = {'treeBagging__n_estimators': [10,20,50], 'treeBagging__max_samples': [0.2,0.5,0.6,0.9]}\n",
        "grid = GridSearchCV(pipe, param_grid=parameteres, cv=5)\n",
        "grid.fit(X_train,y_train)\n",
        "print('CV score for bagging with decision tree is :', grid.score(X_test,y_test))\n",
        "print('Best hyperparameters: ',grid.best_params_)\n",
        "\n",
        "# Random Forest\n",
        "scaler = StandardScaler()\n",
        "RF = RandomForestRegressor()\n",
        "pipe = Pipeline(steps=[('scaler', scaler), ('RF', RF)])\n",
        "parameteres = {'RF__n_estimators': [10,20,50], 'RF__min_samples_split': [2,5,10], 'RF__min_samples_leaf': [2,5,10],'RF__max_samples': [0.2,0.5,0.6]}\n",
        "grid = GridSearchCV(pipe, param_grid=parameteres, cv=5)\n",
        "grid.fit(X_train,y_train)\n",
        "print('CV score for random forest is :', grid.score(X_test,y_test))\n",
        "print('Best hyperparameters: ',grid.best_params_)\n",
        "\n",
        "# Adaboost\n",
        "scaler = StandardScaler()\n",
        "adaboost = AdaBoostRegressor()\n",
        "pipe = Pipeline(steps=[('scaler', scaler), ('adaboost', adaboost)])\n",
        "parameteres = {'adaboost__n_estimators': [10,20,50]}\n",
        "grid = GridSearchCV(pipe, param_grid=parameteres, cv=5)\n",
        "grid.fit(X_train,y_train)\n",
        "print('CV score for AdaBoost is :', grid.score(X_test,y_test))\n",
        "print('Best hyperparameters: ',grid.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLKUe_WXsdrS"
      },
      "source": [
        "We can use random Forest to compute importance of features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnsuRd1PexLA"
      },
      "source": [
        "RF = RandomForestRegressor(max_samples=0.6, min_samples_leaf=5, min_samples_split=5, n_estimators=50)\n",
        "RF.fit(X,y)\n",
        "importances = RF.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Print the feature ranking\n",
        "print(\"Feature ranking:\")\n",
        "\n",
        "for f in range(X.shape[1]):\n",
        "    print(\"%d. feature %s (%f)\" % (f + 1, X.columns[indices[f]], importances[indices[f]]))\n",
        "\n",
        "# Plot the impurity-based feature importances of the forest\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.title(\"Feature importances\")\n",
        "plt.bar(range(X.shape[1]), importances[indices],\n",
        "        color=\"r\", align=\"center\")\n",
        "plt.xticks(range(X.shape[1]), X.columns[indices])\n",
        "plt.xlim([-1, X.shape[1]])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flbp97Nwtg0P"
      },
      "source": [
        "We can see that the most important features are `smoker` and `age`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qjx3xqlOsbEV"
      },
      "source": [
        "**Question**: it's still not clear whether the region features are important or not. And what about the number of children ? Try to answer these questions by squeezing a little bit more the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PndJZmjQtFt-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}